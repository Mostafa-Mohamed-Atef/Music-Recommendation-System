{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1b82a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# PySpark version - no additional pip installs needed if Spark is configured\n",
    "# Spark should be available via docker-compose or local installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6e6dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/os/Desktop/Last.fm dataset/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, explode, split, regexp_replace, lower, trim,\n",
    "    collect_list, array, array_join, udf, when, isnan, isnull,\n",
    "    sum as spark_sum, count, avg, max as spark_max, min as spark_min,\n",
    "    row_number, lit, array_contains, size, regexp_extract\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, DoubleType,\n",
    "    ArrayType, MapType, FloatType\n",
    ")\n",
    "from pyspark.ml.feature import (\n",
    "    Tokenizer, StopWordsRemover, CountVectorizer, IDF, \n",
    "    VectorAssembler, Normalizer, StringIndexer\n",
    ")\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors, SparseVector, DenseVector\n",
    "from pyspark.sql.window import Window\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831259e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Initialize Spark Session\n",
    "# -------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MusicRecommendationSystem\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"✓ Spark session initialized\")\n",
    "\n",
    "# -------------------------\n",
    "# 1. Load data functions (PySpark)\n",
    "# -------------------------\n",
    "def load_interactions_spark(spark: SparkSession, path: str, hdfs_path: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Load tab-delimited (user, track_id, playcount) from HDFS or local path.\n",
    "    If hdfs_path is provided, reads from HDFS, otherwise reads from local path.\n",
    "    \"\"\"\n",
    "    if hdfs_path:\n",
    "        full_path = f\"hdfs://namenode:9000{hdfs_path}\"\n",
    "        print(f\"Loading interactions from HDFS: {full_path}\")\n",
    "    else:\n",
    "        full_path = path\n",
    "        print(f\"Loading interactions from local: {full_path}\")\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"user\", StringType(), True),\n",
    "        StructField(\"track_id\", StringType(), True),\n",
    "        StructField(\"playcount\", DoubleType(), True)\n",
    "    ])\n",
    "    \n",
    "    df = spark.read \\\n",
    "        .option(\"sep\", \"\\t\") \\\n",
    "        .option(\"header\", \"false\") \\\n",
    "        .schema(schema) \\\n",
    "        .csv(full_path)\n",
    "    \n",
    "    # Filter out null values\n",
    "    df = df.filter(col(\"user\").isNotNull() & col(\"track_id\").isNotNull() & col(\"playcount\").isNotNull())\n",
    "    \n",
    "    print(f\"✓ Loaded {df.count():,} interactions\")\n",
    "    print(f\"  - Unique users: {df.select('user').distinct().count():,}\")\n",
    "    print(f\"  - Unique tracks: {df.select('track_id').distinct().count():,}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_tracks_from_hdfs(spark: SparkSession, hdfs_path: str = \"/data/lastfm_data.csv\"):\n",
    "    \"\"\"\n",
    "    Load track metadata from HDFS CSV file (created from JSON files).\n",
    "    Returns Spark DataFrame with normalized track metadata.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[STEP 1] Loading tracks metadata from HDFS...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    full_path = f\"hdfs://namenode:9000{hdfs_path}\"\n",
    "    print(f\"Reading from: {full_path}\")\n",
    "    \n",
    "    # Read CSV\n",
    "    df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(full_path)\n",
    "    \n",
    "    print(f\"✓ Tracks CSV loaded successfully\")\n",
    "    print(f\"  - Total records: {df.count():,}\")\n",
    "    print(f\"  - Columns: {df.columns}\")\n",
    "    \n",
    "    # Parse tags and similars from string representations\n",
    "    # Tags: convert string representation to array\n",
    "    def parse_tags_udf(tags_str):\n",
    "        \"\"\"Parse tags from string representation\"\"\"\n",
    "        if not tags_str or tags_str == '':\n",
    "            return []\n",
    "        try:\n",
    "            parsed = eval(tags_str) if isinstance(tags_str, str) else tags_str\n",
    "            if isinstance(parsed, list):\n",
    "                return [str(t[0]) if isinstance(t, (list, tuple)) and len(t) > 0 else str(t) \n",
    "                       for t in parsed if t]\n",
    "            return []\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    parse_tags = udf(parse_tags_udf, ArrayType(StringType()))\n",
    "    \n",
    "    # Similars: convert to array of tuples (track_id, score)\n",
    "    def parse_similars_udf(similars_str):\n",
    "        \"\"\"Parse similars from string representation\"\"\"\n",
    "        if not similars_str or similars_str == '':\n",
    "            return []\n",
    "        try:\n",
    "            parsed = eval(similars_str) if isinstance(similars_str, str) else similars_str\n",
    "            if isinstance(parsed, list):\n",
    "                result = []\n",
    "                for s in parsed:\n",
    "                    if isinstance(s, (list, tuple)) and len(s) >= 2:\n",
    "                        result.append((str(s[0]), float(s[1])))\n",
    "                return result\n",
    "            return []\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    parse_similars = udf(parse_similars_udf, ArrayType(StructType([\n",
    "        StructField(\"track_id\", StringType()),\n",
    "        StructField(\"score\", DoubleType())\n",
    "    ])))\n",
    "    \n",
    "    # Apply parsing\n",
    "    df = df.withColumn(\"tag_list\", parse_tags(col(\"tags\"))) \\\n",
    "           .withColumn(\"similars_parsed\", parse_similars(col(\"similars\")))\n",
    "    \n",
    "    # Fill nulls\n",
    "    df = df.fillna(\"\", subset=[\"artist\", \"title\"]) \\\n",
    "           .fillna([], subset=[\"tag_list\", \"similars_parsed\"])\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df = df.dropDuplicates([\"track_id\"])\n",
    "    \n",
    "    print(f\"✓ Tags and similars processed\")\n",
    "    print(f\"  - Final records: {df.count():,}\")\n",
    "    print(f\"✓ Track metadata loading complete!\\n\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de87719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Loading Track Metadata ==========\n",
      "Found 617295 JSON files to load...\n",
      "Loaded 10000/617295 files...\n",
      "Loaded 20000/617295 files...\n",
      "Loaded 30000/617295 files...\n",
      "Loaded 40000/617295 files...\n",
      "Loaded 50000/617295 files...\n",
      "Loaded 60000/617295 files...\n",
      "Loaded 70000/617295 files...\n",
      "Loaded 80000/617295 files...\n",
      "Loaded 90000/617295 files...\n",
      "Loaded 100000/617295 files...\n",
      "Loaded 110000/617295 files...\n",
      "Loaded 120000/617295 files...\n",
      "Loaded 130000/617295 files...\n",
      "Loaded 140000/617295 files...\n",
      "Loaded 150000/617295 files...\n",
      "Loaded 160000/617295 files...\n",
      "Loaded 170000/617295 files...\n",
      "Loaded 180000/617295 files...\n",
      "Loaded 190000/617295 files...\n",
      "Loaded 200000/617295 files...\n",
      "Loaded 210000/617295 files...\n",
      "Loaded 220000/617295 files...\n",
      "Error loading lastfm_train/S/D/F/TRSDFHO12903CA5A09.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/F/TRSDFDK128F427A947.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/F/TRSDFOV128F4245F72.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/F/TRSDFMW12903CB4132.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/F/TRSDFOJ128F92F78A7.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/F/TRSDFPA128F9339B9F.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/F/TRSDFOY12903CB5709.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/F/TRSDFLO128F4290592.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/F/TRSDFBV128F4227A71.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/F/TRSDFBY128F14A8E24.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/F/TRSDFLT128E0784590.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/F/TRSDFSY128F92EF96C.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/F/TRSDFOP128EF3408EC.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/F/TRSDFCT128F4282433.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/Q/TRSDQKP128F14ABB35.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/Q/TRSDQAA128F1485F05.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/Q/TRSDQDW128F145BA50.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/Q/TRSDQIC128F93133EE.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/Q/TRSDQGX128F935A145.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/Q/TRSDQGQ128F429017B.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/Q/TRSDQSQ128F429AE05.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/Q/TRSDQHJ128F933A4AD.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/Q/TRSDQOS12903CEDF3E.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/Q/TRSDQSI128F934E8C9.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/Q/TRSDQZH128F9333F84.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/Q/TRSDQYH12903CBAE24.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/Q/TRSDQCL128F93223C9.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/Q/TRSDQYP128F422694C.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/Q/TRSDQWB128EF3563BC.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/Q/TRSDQON128F424E1BF.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/Q/TRSDQNU128F42B0695.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/Q/TRSDQFC128F424860E.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/Q/TRSDQIQ12903CFBDB2.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/Q/TRSDQTP12903CE7B53.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading lastfm_train/S/D/Q/TRSDQXE128F427F4EF.json: Expecting value: line 1 column 1 (char 0)\n",
      "Loaded 230000/617295 files...\n",
      "Loaded 240000/617295 files...\n",
      "Loaded 250000/617295 files...\n",
      "Loaded 260000/617295 files...\n",
      "Loaded 270000/617295 files...\n",
      "Loaded 280000/617295 files...\n",
      "Loaded 290000/617295 files...\n",
      "Loaded 300000/617295 files...\n",
      "Loaded 310000/617295 files...\n",
      "Loaded 320000/617295 files...\n",
      "Loaded 330000/617295 files...\n",
      "Loaded 340000/617295 files...\n",
      "Loaded 350000/617295 files...\n",
      "Loaded 360000/617295 files...\n",
      "Loaded 370000/617295 files...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 130\u001b[39m\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tracks_df\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    129\u001b[39m     \u001b[38;5;66;03m# 1. Load JSON-based track metadata\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     tracks_df = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m     \u001b[38;5;66;03m# 2. Load user-song interactions and run pipeline\u001b[39;00m\n\u001b[32m    133\u001b[39m     interactions_path = \u001b[33m\"\u001b[39m\u001b[33mtrain_triplets.txt\u001b[39m\u001b[33m\"\u001b[39m   \u001b[38;5;66;03m# Your (user, song, playcount)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     97\u001b[39m loader = HDFSDataLoader(hdfs_path)\n\u001b[32m     99\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m========== Loading Track Metadata ==========\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_all_json_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# Stats\u001b[39;00m\n\u001b[32m    103\u001b[39m stats = loader.get_statistics()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mHDFSDataLoader.load_all_json_files\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m         data = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m         \u001b[38;5;28mself\u001b[39m.data.append(data)\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (idx + \u001b[32m1\u001b[39m) % \u001b[32m10000\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/json/__init__.py:293\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(fp, *, \u001b[38;5;28mcls\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m, object_hook=\u001b[38;5;28;01mNone\u001b[39;00m, parse_float=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    275\u001b[39m         parse_int=\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant=\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook=\u001b[38;5;28;01mNone\u001b[39;00m, **kw):\n\u001b[32m    276\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[32m    278\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m \u001b[33;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/json/decoder.py:346\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[33;03mcontaining a JSON document).\u001b[39;00m\n\u001b[32m    343\u001b[39m \n\u001b[32m    344\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    345\u001b[39m obj, end = \u001b[38;5;28mself\u001b[39m.raw_decode(s, idx=_w(s, \u001b[32m0\u001b[39m).end())\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m end = \u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m.end()\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExtra data\u001b[39m\u001b[33m\"\u001b[39m, s, end)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Build track dictionary (PySpark version)\n",
    "# -------------------------\n",
    "def build_track_dict_spark(tracks_df):\n",
    "    \"\"\"\n",
    "    Build track metadata dictionary from Spark DataFrame.\n",
    "    Returns a broadcast dictionary for efficient lookups.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[STEP 2] Building track metadata dictionary...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Collect track metadata to driver (for small-medium datasets)\n",
    "    # For very large datasets, consider using broadcast joins instead\n",
    "    tracks_pd = tracks_df.select(\n",
    "        \"track_id\", \"artist\", \"title\", \"tag_list\", \"similars_parsed\"\n",
    "    ).toPandas()\n",
    "    \n",
    "    track_dict = {}\n",
    "    invalid_count = 0\n",
    "    \n",
    "    for _, row in tracks_pd.iterrows():\n",
    "        tid = row.get('track_id')\n",
    "        if not tid or pd.isna(tid):\n",
    "            invalid_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Convert similars_parsed to list of tuples\n",
    "        similars = []\n",
    "        if row.get('similars_parsed'):\n",
    "            try:\n",
    "                if isinstance(row['similars_parsed'], list):\n",
    "                    similars = [(str(s[0]), float(s[1])) if isinstance(s, (list, tuple)) and len(s) >= 2 \n",
    "                               else (str(s), 0.0) for s in row['similars_parsed']]\n",
    "            except:\n",
    "                similars = []\n",
    "        \n",
    "        track_dict[str(tid)] = {\n",
    "            'artist': str(row.get('artist', '')) if not pd.isna(row.get('artist', '')) else '',\n",
    "            'title': str(row.get('title', '')) if not pd.isna(row.get('title', '')) else '',\n",
    "            'tags': [str(t) for t in (row.get('tag_list', []) or [])],\n",
    "            'similars': similars\n",
    "        }\n",
    "    \n",
    "    print(f\"✓ Track dict built successfully\")\n",
    "    print(f\"  - Valid tracks indexed: {len(track_dict):,}\")\n",
    "    print(f\"  - Invalid/missing track_ids skipped: {invalid_count}\")\n",
    "    print(f\"✓ Track metadata dictionary complete!\\n\")\n",
    "    \n",
    "    # Broadcast for efficient distributed access\n",
    "    return spark.sparkContext.broadcast(track_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c0e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load tracks from HDFS\n",
    "# tracks_df = load_tracks_from_hdfs(spark, hdfs_path=\"/data/lastfm_data.csv\")\n",
    "# Or load from local if testing:\n",
    "# tracks_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"tracks.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c57394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>similars</th>\n",
       "      <th>tags</th>\n",
       "      <th>track_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Computer Truck</td>\n",
       "      <td>2011-08-08 18:30:39.698667</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>TRRRRIR128F933C8FE</td>\n",
       "      <td>Gamegirl power</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lack of Limits</td>\n",
       "      <td>2011-08-02 10:04:50.589618</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>TRRRRUH128F14ABD68</td>\n",
       "      <td>30 Summers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Kinks</td>\n",
       "      <td>2011-08-02 05:18:14.083535</td>\n",
       "      <td>[['TRMLOXQ12903CF06BB', 1], ['TRCOWHF128F93216...</td>\n",
       "      <td>[['classic rock', '100'], ['60s', '76'], ['bri...</td>\n",
       "      <td>TRRRRCH128F9342C72</td>\n",
       "      <td>A Well Respected Man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Everclear</td>\n",
       "      <td>2011-08-15 10:19:29.458998</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>TRRRRNT128E0782AF2</td>\n",
       "      <td>Misery Whip (Explicit)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Paula Abdul</td>\n",
       "      <td>2011-08-04 11:50:10.764331</td>\n",
       "      <td>[['TRALFWK128F1458532', 1], ['TRRMRMZ128F14585...</td>\n",
       "      <td>[['pop', '100'], ['female vocalists', '66'], [...</td>\n",
       "      <td>TRRRRMR128F145852B</td>\n",
       "      <td>Rock House</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           artist                   timestamp  \\\n",
       "0  Computer Truck  2011-08-08 18:30:39.698667   \n",
       "1  Lack of Limits  2011-08-02 10:04:50.589618   \n",
       "2       The Kinks  2011-08-02 05:18:14.083535   \n",
       "3       Everclear  2011-08-15 10:19:29.458998   \n",
       "4     Paula Abdul  2011-08-04 11:50:10.764331   \n",
       "\n",
       "                                            similars  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2  [['TRMLOXQ12903CF06BB', 1], ['TRCOWHF128F93216...   \n",
       "3                                                 []   \n",
       "4  [['TRALFWK128F1458532', 1], ['TRRMRMZ128F14585...   \n",
       "\n",
       "                                                tags            track_id  \\\n",
       "0                                                 []  TRRRRIR128F933C8FE   \n",
       "1                                                 []  TRRRRUH128F14ABD68   \n",
       "2  [['classic rock', '100'], ['60s', '76'], ['bri...  TRRRRCH128F9342C72   \n",
       "3                                                 []  TRRRRNT128E0782AF2   \n",
       "4  [['pop', '100'], ['female vocalists', '66'], [...  TRRRRMR128F145852B   \n",
       "\n",
       "                    title  \n",
       "0          Gamegirl power  \n",
       "1              30 Summers  \n",
       "2    A Well Respected Man  \n",
       "3  Misery Whip (Explicit)  \n",
       "4              Rock House  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display sample tracks\n",
    "# tracks_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d53d3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PySpark Hybrid Recommender System\n",
    "Uses Spark MLlib for:\n",
    "- ALS Collaborative Filtering\n",
    "- TF-IDF Content-Based Features  \n",
    "- FP-Growth Association Rules\n",
    "\n",
    "Data sources:\n",
    "- HDFS: /data/lastfm_data.csv (track metadata)\n",
    "- HDFS: /data/train_triplets.txt (user-track interactions)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e7bef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# --- Build user/item mappings with StringIndexer ---\n",
    "# ---------------------------\n",
    "\n",
    "def build_mappings_and_user_item_spark(interactions_df, spark: SparkSession):\n",
    "    \"\"\"\n",
    "    Build user/item indexers and prepare interactions DataFrame for ALS.\n",
    "    Returns indexed interactions DataFrame and indexer models.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[STEP 3] Building user/item mappings and interaction matrix...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"\\n[Processing] Creating user and item indexers...\")\n",
    "    \n",
    "    # Create StringIndexers for users and items\n",
    "    user_indexer = StringIndexer(inputCol=\"user\", outputCol=\"user_idx\", handleInvalid=\"skip\")\n",
    "    item_indexer = StringIndexer(inputCol=\"track_id\", outputCol=\"item_idx\", handleInvalid=\"skip\")\n",
    "    \n",
    "    # Fit indexers\n",
    "    print(f\"  - Fitting user indexer...\")\n",
    "    user_indexer_model = user_indexer.fit(interactions_df)\n",
    "    print(f\"  - Fitting item indexer...\")\n",
    "    item_indexer_model = item_indexer.fit(interactions_df)\n",
    "    \n",
    "    # Transform interactions\n",
    "    print(f\"  - Transforming interactions with indexers...\")\n",
    "    interactions_indexed = user_indexer_model.transform(interactions_df)\n",
    "    interactions_indexed = item_indexer_model.transform(interactions_indexed)\n",
    "    \n",
    "    # Filter out any null indices\n",
    "    interactions_indexed = interactions_indexed.filter(\n",
    "        col(\"user_idx\").isNotNull() & col(\"item_idx\").isNotNull()\n",
    "    )\n",
    "    \n",
    "    # Convert indices to integer\n",
    "    interactions_indexed = interactions_indexed.withColumn(\"user_idx\", col(\"user_idx\").cast(IntegerType())) \\\n",
    "                                               .withColumn(\"item_idx\", col(\"item_idx\").cast(IntegerType()))\n",
    "    \n",
    "    # Get statistics\n",
    "    total_interactions = interactions_indexed.count()\n",
    "    unique_users = interactions_indexed.select(\"user_idx\").distinct().count()\n",
    "    unique_items = interactions_indexed.select(\"item_idx\").distinct().count()\n",
    "    \n",
    "    print(f\"\\n✓ Indexing complete!\")\n",
    "    print(f\"  - Unique users: {unique_users:,}\")\n",
    "    print(f\"  - Unique items: {unique_items:,}\")\n",
    "    print(f\"  - Total interactions: {total_interactions:,}\")\n",
    "    \n",
    "    # Get mappings\n",
    "    user_mapping = interactions_indexed.select(\"user\", \"user_idx\").distinct().collect()\n",
    "    item_mapping = interactions_indexed.select(\"track_id\", \"item_idx\").distinct().collect()\n",
    "    \n",
    "    user2idx = {row.user: int(row.user_idx) for row in user_mapping}\n",
    "    item2idx = {row.track_id: int(row.item_idx) for row in item_mapping}\n",
    "    idx2user = {v: k for k, v in user2idx.items()}\n",
    "    idx2item = {v: k for k, v in item2idx.items()}\n",
    "    \n",
    "    print(f\"✓ User/item mapping complete!\\n\")\n",
    "    \n",
    "    return interactions_indexed, user2idx, item2idx, idx2user, idx2item, user_indexer_model, item_indexer_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d00512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# --- TF-IDF content vectors (Spark MLlib) ---\n",
    "# ---------------------------\n",
    "\n",
    "def build_tfidf_content_matrix_spark(tracks_df, item2idx: Dict[str, int], max_features: int = 50000):\n",
    "    \"\"\"\n",
    "    Build TF-IDF content matrix using Spark MLlib.\n",
    "    Returns TF-IDF model and transformed features DataFrame.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[STEP 4] Building TF-IDF content matrix...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"\\n[Processing] Constructing text corpus from track metadata...\")\n",
    "    \n",
    "    # Create text column by combining artist, title, and tags\n",
    "    def combine_text(artist, title, tags):\n",
    "        artist_str = str(artist) if artist else \"\"\n",
    "        title_str = str(title) if title else \"\"\n",
    "        tags_str = \" \".join([str(t) for t in (tags or [])])\n",
    "        return \" \".join([artist_str, title_str, tags_str]).strip()\n",
    "    \n",
    "    combine_text_udf = udf(combine_text, StringType())\n",
    "    \n",
    "    # Join tracks with item indices\n",
    "    tracks_with_idx = tracks_df.withColumn(\n",
    "        \"text\", combine_text_udf(col(\"artist\"), col(\"title\"), col(\"tag_list\"))\n",
    "    ).filter(col(\"text\") != \"\")\n",
    "    \n",
    "    # Index tracks by item_idx for alignment\n",
    "    item_idx_df = spark.createDataFrame(\n",
    "        [(tid, idx) for tid, idx in item2idx.items()],\n",
    "        [\"track_id\", \"item_idx\"]\n",
    "    )\n",
    "    \n",
    "    tracks_indexed = tracks_with_idx.join(item_idx_df, on=\"track_id\", how=\"inner\")\n",
    "    \n",
    "    print(f\"  - Tracks with text: {tracks_indexed.count():,}\")\n",
    "    \n",
    "    # Tokenize\n",
    "    print(f\"\\n[Training] Tokenizing and building TF-IDF (max_features={max_features:,})...\")\n",
    "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "    words_df = tokenizer.transform(tracks_indexed)\n",
    "    \n",
    "    # CountVectorizer (term frequency)\n",
    "    cv = CountVectorizer(\n",
    "        inputCol=\"words\",\n",
    "        outputCol=\"raw_features\",\n",
    "        vocabSize=max_features,\n",
    "        minDF=1.0\n",
    "    )\n",
    "    cv_model = cv.fit(words_df)\n",
    "    cv_df = cv_model.transform(words_df)\n",
    "    \n",
    "    print(f\"  - Vocabulary size: {len(cv_model.vocabulary):,}\")\n",
    "    \n",
    "    # IDF\n",
    "    idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_features\")\n",
    "    idf_model = idf.fit(cv_df)\n",
    "    tfidf_df = idf_model.transform(cv_df)\n",
    "    \n",
    "    # Normalize (L2 normalization)\n",
    "    normalizer = Normalizer(inputCol=\"tfidf_features\", outputCol=\"tfidf_normalized\", p=2.0)\n",
    "    tfidf_normalized_df = normalizer.transform(tfidf_df)\n",
    "    \n",
    "    print(f\"\\n✓ TF-IDF matrix built successfully!\")\n",
    "    print(f\"  - Tracks processed: {tfidf_normalized_df.count():,}\")\n",
    "    print(f\"✓ Content matrix building complete!\\n\")\n",
    "    \n",
    "    return tfidf_normalized_df, cv_model, idf_model, normalizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5e8149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# --- FP-Growth association rules (Spark MLlib) ---\n",
    "# ---------------------------\n",
    "\n",
    "def build_fpgrowth_rules_spark(interactions_indexed_df,\n",
    "                               min_support: float = 0.001,\n",
    "                               sample_frac: float = 0.2,\n",
    "                               min_confidence: float = 0.2):\n",
    "    \"\"\"\n",
    "    Build frequent itemsets & association rules using Spark MLlib FP-Growth.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[STEP 5] Building FP-Growth association rules...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  - min_support: {min_support}\")\n",
    "    print(f\"  - sample_frac: {sample_frac}\")\n",
    "    print(f\"  - min_confidence: {min_confidence}\")\n",
    "    \n",
    "    # Sample transactions if needed\n",
    "    if sample_frac < 1.0:\n",
    "        print(f\"\\n[Sampling] Sampling {sample_frac*100:.1f}% of transactions...\")\n",
    "        interactions_sampled = interactions_indexed_df.sample(False, sample_frac, seed=42)\n",
    "    else:\n",
    "        interactions_sampled = interactions_indexed_df\n",
    "    \n",
    "    # Group by user to create transactions (itemsets)\n",
    "    print(f\"\\n[Processing] Grouping items by user to create transactions...\")\n",
    "    user_itemsets = interactions_sampled.groupBy(\"user_idx\") \\\n",
    "        .agg(collect_list(\"item_idx\").alias(\"items\"))\n",
    "    \n",
    "    # Convert to array of integers (required by FP-Growth)\n",
    "    user_itemsets = user_itemsets.select(\"items\")\n",
    "    \n",
    "    print(f\"  - Transactions created: {user_itemsets.count():,}\")\n",
    "    \n",
    "    # Run FP-Growth\n",
    "    print(f\"\\n[Training] Running FP-Growth algorithm...\")\n",
    "    fpgrowth_model = FPGrowth(\n",
    "        itemsCol=\"items\",\n",
    "        minSupport=min_support,\n",
    "        minConfidence=min_confidence\n",
    "    )\n",
    "    \n",
    "    model = fpgrowth_model.fit(user_itemsets)\n",
    "    \n",
    "    # Get frequent itemsets\n",
    "    freq_itemsets = model.freqItemsets\n",
    "    print(f\"✓ Frequent itemsets generated: {freq_itemsets.count():,}\")\n",
    "    \n",
    "    # Get association rules\n",
    "    rules_df = model.associationRules\n",
    "    print(f\"✓ Association rules generated: {rules_df.count():,}\")\n",
    "    \n",
    "    if rules_df.count() > 0:\n",
    "        rules_stats = rules_df.agg(\n",
    "            spark_min(\"confidence\").alias(\"min_conf\"),\n",
    "            spark_max(\"confidence\").alias(\"max_conf\"),\n",
    "            spark_min(\"lift\").alias(\"min_lift\"),\n",
    "            spark_max(\"lift\").alias(\"max_lift\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f\"  - Confidence range: [{rules_stats.min_conf:.4f}, {rules_stats.max_conf:.4f}]\")\n",
    "        print(f\"  - Lift range: [{rules_stats.min_lift:.4f}, {rules_stats.max_lift:.4f}]\")\n",
    "    \n",
    "    print(f\"✓ FP-Growth rules generation complete!\\n\")\n",
    "    \n",
    "    return rules_df, freq_itemsets, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f515c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# --- ALS training (Spark MLlib) ---\n",
    "# ---------------------------\n",
    "\n",
    "def train_als_model_spark(interactions_indexed_df,\n",
    "                         factors: int = 64,\n",
    "                         regularization: float = 0.01,\n",
    "                         iterations: int = 15,\n",
    "                         implicit_prefs: bool = True,\n",
    "                         alpha: float = 40.0):\n",
    "    \"\"\"\n",
    "    Train ALS model using Spark MLlib.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[STEP 6] Training Spark MLlib ALS Model...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  - Factors (latent dimensions): {factors}\")\n",
    "    print(f\"  - Regularization: {regularization}\")\n",
    "    print(f\"  - Iterations: {iterations}\")\n",
    "    print(f\"  - Implicit preferences: {implicit_prefs}\")\n",
    "    if implicit_prefs:\n",
    "        print(f\"  - Alpha (confidence scaling): {alpha}\")\n",
    "    \n",
    "    # Prepare data for ALS (user_idx, item_idx, rating)\n",
    "    als_data = interactions_indexed_df.select(\n",
    "        col(\"user_idx\").alias(\"user\"),\n",
    "        col(\"item_idx\").alias(\"item\"),\n",
    "        col(\"playcount\").alias(\"rating\")\n",
    "    )\n",
    "    \n",
    "    # If implicit, transform ratings\n",
    "    if implicit_prefs:\n",
    "        als_data = als_data.withColumn(\"rating\", (1.0 + alpha * col(\"rating\")).cast(DoubleType()))\n",
    "    \n",
    "    print(f\"  - Training interactions: {als_data.count():,}\")\n",
    "    \n",
    "    # Create ALS model\n",
    "    als = ALS(\n",
    "        maxIter=iterations,\n",
    "        regParam=regularization,\n",
    "        rank=factors,\n",
    "        implicitPrefs=implicit_prefs,\n",
    "        userCol=\"user\",\n",
    "        itemCol=\"item\",\n",
    "        ratingCol=\"rating\",\n",
    "        coldStartStrategy=\"drop\",\n",
    "        nonnegative=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n[Training] Fitting ALS model (this may take a while)...\")\n",
    "    model = als.fit(als_data)\n",
    "    \n",
    "    print(f\"\\n✓ ALS training finished successfully!\")\n",
    "    print(f\"✓ ALS model training complete!\\n\")\n",
    "    \n",
    "    return model, als_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02bc9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# --- Helper functions for similarity calculations ---\n",
    "# ---------------------------\n",
    "\n",
    "def compute_item_similarities_spark(interactions_indexed_df, item2idx: Dict[str, int]):\n",
    "    \"\"\"\n",
    "    Precompute item-item similarities using co-occurrence and cosine similarity.\n",
    "    Returns Spark DataFrame with item similarities.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[STEP 7] Computing item-item similarities...\")\n",
    "    \n",
    "    # Co-occurrence: items that appear together in user playlists\n",
    "    user_items = interactions_indexed_df.select(\"user_idx\", \"item_idx\").distinct()\n",
    "    \n",
    "    # Self-join to find co-occurrences\n",
    "    item1 = user_items.alias(\"item1\")\n",
    "    item2 = user_items.alias(\"item2\")\n",
    "    \n",
    "    cooccurrences = item1.join(\n",
    "        item2,\n",
    "        (col(\"item1.user_idx\") == col(\"item2.user_idx\")) & \n",
    "        (col(\"item1.item_idx\") != col(\"item2.item_idx\")),\n",
    "        \"inner\"\n",
    "    ).select(\n",
    "        col(\"item1.item_idx\").alias(\"item_idx_1\"),\n",
    "        col(\"item2.item_idx\").alias(\"item_idx_2\")\n",
    "    ).groupBy(\"item_idx_1\", \"item_idx_2\").count().withColumnRenamed(\"count\", \"cooccurrence_count\")\n",
    "    \n",
    "    print(f\"✓ Co-occurrence matrix computed: {cooccurrences.count():,} pairs\")\n",
    "    \n",
    "    return cooccurrences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67142981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# --- Hybrid Recommender Class (PySpark) ---\n",
    "# ---------------------------\n",
    "\n",
    "class HybridRecommenderSpark:\n",
    "    \"\"\"\n",
    "    Hybrid recommender system using PySpark.\n",
    "    Combines ALS collaborative filtering, content-based (TF-IDF), and association rules.\n",
    "    \"\"\"\n",
    "    def __init__(self, spark: SparkSession,\n",
    "                 interactions_indexed_df,\n",
    "                 user2idx: Dict[str, int],\n",
    "                 item2idx: Dict[str, int],\n",
    "                 idx2user: Dict[int, str],\n",
    "                 idx2item: Dict[int, str],\n",
    "                 track_dict_broadcast,\n",
    "                 als_model=None,\n",
    "                 tfidf_df=None,\n",
    "                 rules_df=None,\n",
    "                 cooccurrences_df=None):\n",
    "        self.spark = spark\n",
    "        self.interactions_indexed_df = interactions_indexed_df\n",
    "        self.user2idx = user2idx\n",
    "        self.item2idx = item2idx\n",
    "        self.idx2user = idx2user\n",
    "        self.idx2item = idx2item\n",
    "        self.track_dict_broadcast = track_dict_broadcast\n",
    "        self.als_model = als_model\n",
    "        self.tfidf_df = tfidf_df\n",
    "        self.rules_df = rules_df\n",
    "        self.cooccurrences_df = cooccurrences_df\n",
    "    \n",
    "    def recommend_for_user(self, user_id: str, top_k: int = 10, weights: Dict[str, float] = None):\n",
    "        \"\"\"\n",
    "        Generate recommendations for a user.\n",
    "        Returns list of (track_id, score) tuples.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = {'als': 0.6, 'coocc': 0.25, 'content': 0.15}\n",
    "        \n",
    "        if user_id not in self.user2idx:\n",
    "            # Cold start: return popular items\n",
    "            popular = self.interactions_indexed_df.groupBy(\"item_idx\") \\\n",
    "                .agg(spark_sum(\"playcount\").alias(\"total_plays\")) \\\n",
    "                .orderBy(col(\"total_plays\").desc()) \\\n",
    "                .limit(top_k) \\\n",
    "                .collect()\n",
    "            return [(self.idx2item[row.item_idx], float(row.total_plays)) for row in popular]\n",
    "        \n",
    "        user_idx = self.user2idx[user_id]\n",
    "        scores = defaultdict(float)\n",
    "        \n",
    "        # ALS recommendations\n",
    "        if self.als_model is not None:\n",
    "            try:\n",
    "                # Create DataFrame with user_idx\n",
    "                user_df = self.spark.createDataFrame([(user_idx,)], [\"user\"])\n",
    "                als_recs = self.als_model.recommendForUserSubset(user_df, top_k * 5)\n",
    "                \n",
    "                recs = als_recs.select(\"recommendations\").collect()\n",
    "                if recs:\n",
    "                    for rec_row in recs[0].recommendations:\n",
    "                        item_idx = rec_row.item\n",
    "                        score = rec_row.rating\n",
    "                        track_id = self.idx2item.get(item_idx)\n",
    "                        if track_id:\n",
    "                            scores[track_id] += weights['als'] * float(score)\n",
    "            except Exception as e:\n",
    "                print(f\"ALS recommendation error: {e}\")\n",
    "        \n",
    "        # Get user's listened items\n",
    "        user_items = self.interactions_indexed_df.filter(col(\"user_idx\") == user_idx) \\\n",
    "            .select(\"item_idx\").distinct().collect()\n",
    "        user_item_set = {row.item_idx for row in user_items}\n",
    "        \n",
    "        # Co-occurrence recommendations\n",
    "        if self.cooccurrences_df is not None and user_item_set:\n",
    "            for item_idx in list(user_item_set)[:100]:  # Limit for performance\n",
    "                coocc_recs = self.cooccurrences_df.filter(col(\"item_idx_1\") == item_idx) \\\n",
    "                    .orderBy(col(\"cooccurrence_count\").desc()) \\\n",
    "                    .limit(50) \\\n",
    "                    .collect()\n",
    "                for row in coocc_recs:\n",
    "                    track_id = self.idx2item.get(row.item_idx_2)\n",
    "                    if track_id and track_id not in user_item_set:\n",
    "                        scores[track_id] += weights['coocc'] * float(row.cooccurrence_count) / len(user_item_set)\n",
    "        \n",
    "        # Content-based recommendations (simplified - use TF-IDF similarity)\n",
    "        # This would require more complex vector similarity calculations\n",
    "        \n",
    "        # Remove already listened items\n",
    "        listened_tracks = {self.idx2item[i] for i in user_item_set}\n",
    "        final_scores = [(tid, sc) for tid, sc in scores.items() if tid not in listened_tracks]\n",
    "        final_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return final_scores[:top_k]\n",
    "    \n",
    "    def recommend_similar_song(self, track_id: str, top_k: int = 10, weights: Dict[str, float] = None):\n",
    "        \"\"\"\n",
    "        Find similar songs to a given track.\n",
    "        Returns list of (track_id, score) tuples.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = {'similars': 0.4, 'coocc': 0.2, 'content': 0.2}\n",
    "        \n",
    "        scores = defaultdict(float)\n",
    "        track_dict = self.track_dict_broadcast.value\n",
    "        \n",
    "        # Metadata similars\n",
    "        md = track_dict.get(track_id, {})\n",
    "        for sid, sscore in md.get('similars', []):\n",
    "            scores[sid] += weights['similars'] * float(sscore)\n",
    "        \n",
    "        # Item index\n",
    "        item_idx = self.item2idx.get(track_id)\n",
    "        if item_idx is not None:\n",
    "            # Co-occurrence\n",
    "            if self.cooccurrences_df is not None:\n",
    "                coocc_recs = self.cooccurrences_df.filter(col(\"item_idx_1\") == item_idx) \\\n",
    "                    .orderBy(col(\"cooccurrence_count\").desc()) \\\n",
    "                    .limit(200) \\\n",
    "                    .collect()\n",
    "                for row in coocc_recs:\n",
    "                    sid = self.idx2item.get(row.item_idx_2)\n",
    "                    if sid:\n",
    "                        scores[sid] += weights['coocc'] * float(row.cooccurrence_count)\n",
    "        \n",
    "        # Remove the input track\n",
    "        scores.pop(track_id, None)\n",
    "        ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return ranked[:top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea892ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# --- Hybrid recommender class ---\n",
    "# ---------------------------\n",
    "\n",
    "class HybridRecommender:\n",
    "    def __init__(self, user_item: csr_matrix, user2idx: Dict[str,int], item2idx: Dict[str,int],\n",
    "                 track_dict: Dict[str,Dict], als_model=None, fallback_mf=None, content_matrix=None,\n",
    "                 tfidf_vectorizer=None, rules_df=None, item_norms=None):\n",
    "        self.user_item = user_item\n",
    "        self.user2idx = user2idx\n",
    "        self.item2idx = item2idx\n",
    "        self.idx2item = {v:k for k,v in item2idx.items()}\n",
    "        self.track_dict = track_dict\n",
    "        self.als_model = als_model\n",
    "        self.fallback_mf = fallback_mf\n",
    "        self.content_matrix = content_matrix\n",
    "        self.tfidf_vectorizer = tfidf_vectorizer\n",
    "        self.rules_df = rules_df\n",
    "        self.item_norms = item_norms if item_norms is not None else precompute_item_norms(user_item)\n",
    "\n",
    "    def recommend_similar_song(self, track_id: str, top_k: int = 10, weights: Dict[str,float] = None):\n",
    "        if weights is None:\n",
    "            weights = {'similars': 0.4, 'coocc': 0.2, 'cosine': 0.2, 'content': 0.2}\n",
    "        scores = defaultdict(float)\n",
    "        # metadata similars\n",
    "        md = self.track_dict.get(track_id, {})\n",
    "        for sid, sscore in md.get('similars', []):\n",
    "            scores[sid] += weights['similars'] * float(sscore)\n",
    "        # item index\n",
    "        item_idx = self.item2idx.get(track_id, None)\n",
    "        if item_idx is not None:\n",
    "            # coocc\n",
    "            for idx, conf in topk_cooccurrence(item_idx, self.user_item, top_k=200):\n",
    "                sid = self.idx2item[idx]\n",
    "                scores[sid] += weights['coocc'] * conf\n",
    "            # cosine\n",
    "            for idx, sc in topk_item_cosine(item_idx, self.user_item, self.item_norms, top_k=200):\n",
    "                sid = self.idx2item[idx]\n",
    "                scores[sid] += weights['cosine'] * sc\n",
    "            # content\n",
    "            if self.content_matrix is not None:\n",
    "                q = self.content_matrix[item_idx]\n",
    "                sims = q.dot(self.content_matrix.T).toarray().ravel()\n",
    "                sims[item_idx] = 0.0\n",
    "                topk = np.argpartition(-sims, min(200, len(sims)-1))[:min(200, len(sims)-1)]\n",
    "                topk = topk[np.argsort(-sims[topk])]\n",
    "                for idx in topk:\n",
    "                    sid = self.idx2item[idx]\n",
    "                    scores[sid] += weights['content'] * float(sims[idx])\n",
    "        # rules: if rules_df exists, use consequents where antecedent subset matches track's similars\n",
    "        if self.rules_df is not None and not self.rules_df.empty:\n",
    "            user_items = set([s[0] for s in md.get('similars', [])])\n",
    "            for _, row in self.rules_df.iterrows():\n",
    "                antecedent = set(row['antecedents'])\n",
    "                consequent = set(row['consequents'])\n",
    "                if antecedent <= user_items:\n",
    "                    for c in consequent:\n",
    "                        scores[c] += 0.1 * row['confidence']\n",
    "        # finalize\n",
    "        scores.pop(track_id, None)\n",
    "        ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return ranked[:top_k]\n",
    "\n",
    "    def recommend_for_user(self, user_id: str, top_k: int = 10, weights: Dict[str,float] = None):\n",
    "        if weights is None:\n",
    "            weights = {'als':0.6, 'coocc':0.25, 'content':0.15}\n",
    "        scored = defaultdict(float)\n",
    "        if user_id not in self.user2idx:\n",
    "            # cold user fallback: popular items\n",
    "            pops = np.array(self.user_item.sum(axis=0)).ravel()\n",
    "            top_idx = np.argpartition(-pops, top_k)[:top_k]\n",
    "            top_idx = top_idx[np.argsort(-pops[top_idx])]\n",
    "            return [(self.idx2item[i], float(pops[i])) for i in top_idx]\n",
    "        uidx = self.user2idx[user_id]\n",
    "        user_items = self.user_item[uidx].nonzero()[1].tolist()\n",
    "        # ALS\n",
    "        if self.als_model is not None:\n",
    "            try:\n",
    "                recs = self.als_model.recommend(uidx, self.user_item, N=top_k*5, filter_already_liked_items=True)\n",
    "                for idx, sc in recs:\n",
    "                    tid = self.idx2item[idx]\n",
    "                    scored[tid] += weights['als'] * float(sc)\n",
    "            except Exception:\n",
    "                pass\n",
    "        elif self.fallback_mf is not None:\n",
    "            # fallback MF: use Surprise predictions (slow)\n",
    "            algo, trainset = self.fallback_mf\n",
    "            user_rated = set()\n",
    "            try:\n",
    "                u_inner = trainset.to_inner_uid(user_id)\n",
    "                user_rated = set([trainset.to_raw_iid(i) for (i,_) in trainset.ur[u_inner]])\n",
    "            except Exception:\n",
    "                user_rated = set()\n",
    "            candidates = [it for it in self.item2idx.keys() if it not in user_rated]\n",
    "            preds=[]\n",
    "            for it in candidates:\n",
    "                try:\n",
    "                    preds.append((it, algo.predict(user_id, it).est))\n",
    "                except:\n",
    "                    continue\n",
    "            preds.sort(key=lambda x: x[1], reverse=True)\n",
    "            for it,score in preds[:top_k*5]:\n",
    "                scored[it] += weights['als'] * float(score)\n",
    "        else:\n",
    "            # no CF models available -> skip ALS part\n",
    "            pass\n",
    "        # cooccurrence aggregated\n",
    "        for ui in user_items:\n",
    "            for idx, conf in topk_cooccurrence(ui, self.user_item, top_k=50):\n",
    "                tid = self.idx2item[idx]\n",
    "                scored[tid] += weights['coocc'] * conf * (1.0 / max(1,len(user_items)))\n",
    "        # content aggregated\n",
    "        if self.content_matrix is not None:\n",
    "            for ui in user_items:\n",
    "                if ui >= self.content_matrix.shape[0]:\n",
    "                    continue\n",
    "                q = self.content_matrix[ui]\n",
    "                sims = q.dot(self.content_matrix.T).toarray().ravel()\n",
    "                topk = np.argpartition(-sims, min(50, len(sims)-1))[:min(50, len(sims)-1)]\n",
    "                topk = topk[np.argsort(-sims[topk])]\n",
    "                for idx in topk:\n",
    "                    tid = self.idx2item[idx]\n",
    "                    scored[tid] += weights['content'] * float(sims[idx]) * (1.0 / max(1,len(user_items)))\n",
    "        # rules-based boost using rules_df if available\n",
    "        if self.rules_df is not None and not self.rules_df.empty:\n",
    "            user_item_ids = set([self.idx2item[i] for i in user_items])\n",
    "            for _, row in self.rules_df.iterrows():\n",
    "                antecedent = set(row['antecedents'])\n",
    "                consequent = set(row['consequents'])\n",
    "                if antecedent <= user_item_ids:\n",
    "                    for c in consequent:\n",
    "                        if c not in user_item_ids:\n",
    "                            scored[c] += 0.1 * row['confidence']\n",
    "        # remove already listened\n",
    "        listened = set([self.idx2item[i] for i in user_items])\n",
    "        final = [(tid, sc) for tid, sc in scored.items() if tid not in listened]\n",
    "        final.sort(key=lambda x: x[1], reverse=True)\n",
    "        return final[:top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cf1d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# --- Full pipeline builder (PySpark) ---\n",
    "# ---------------------------\n",
    "\n",
    "def build_and_run_pipeline_main_spark(spark: SparkSession,\n",
    "                                     interactions_hdfs_path: str = \"/data/train_triplets.txt\",\n",
    "                                     tracks_hdfs_path: str = \"/data/lastfm_data.csv\",\n",
    "                                     fpgrowth_min_support: float = 0.001,\n",
    "                                     fpgrowth_sample_frac: float = 0.2,\n",
    "                                     als_factors: int = 64,\n",
    "                                     als_iterations: int = 15):\n",
    "    print(f\"\\n\\n{'#'*60}\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    print(f\"# PYSPARK HYBRID RECOMMENDATION SYSTEM - FULL PIPELINE\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    print(f\"{'#'*60}\\n\")\n",
    "    \n",
    "    print(f\"[CONFIG] Pipeline parameters:\")\n",
    "    print(f\"  - Interactions HDFS path: {interactions_hdfs_path}\")\n",
    "    print(f\"  - Tracks HDFS path: {tracks_hdfs_path}\")\n",
    "    print(f\"  - FP-Growth min support: {fpgrowth_min_support}\")\n",
    "    print(f\"  - FP-Growth sample fraction: {fpgrowth_sample_frac}\")\n",
    "    print(f\"  - ALS factors: {als_factors}\")\n",
    "    print(f\"  - ALS iterations: {als_iterations}\")\n",
    "    \n",
    "    # 1) Load track metadata from HDFS\n",
    "    print(f\"\\n\\n{'*'*60}\")\n",
    "    print(f\"PHASE 1: LOAD TRACK METADATA FROM HDFS\")\n",
    "    print(f\"{'*'*60}\\n\")\n",
    "    tracks_df = load_tracks_from_hdfs(spark, tracks_hdfs_path)\n",
    "    track_dict_broadcast = build_track_dict_spark(tracks_df)\n",
    "    \n",
    "    # 2) Load interactions and build mappings\n",
    "    print(f\"\\n\\n{'*'*60}\")\n",
    "    print(f\"PHASE 2: LOAD INTERACTIONS AND BUILD MAPPINGS\")\n",
    "    print(f\"{'*'*60}\\n\")\n",
    "    interactions_df = load_interactions_spark(spark, None, interactions_hdfs_path)\n",
    "    interactions_indexed, user2idx, item2idx, idx2user, idx2item, user_idx_model, item_idx_model = \\\n",
    "        build_mappings_and_user_item_spark(interactions_df, spark)\n",
    "    \n",
    "    # 3) Build TF-IDF content features\n",
    "    print(f\"\\n\\n{'*'*60}\")\n",
    "    print(f\"PHASE 3: BUILD CONTENT-BASED FEATURES (TF-IDF)\")\n",
    "    print(f\"{'*'*60}\\n\")\n",
    "    try:\n",
    "        tfidf_df, cv_model, idf_model, normalizer = build_tfidf_content_matrix_spark(\n",
    "            tracks_df, item2idx, max_features=50000\n",
    "        )\n",
    "        print(f\"✓ Content matrix successfully built!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ TF-IDF build failed: {e}\")\n",
    "        print(f\"  - Continuing without content features...\")\n",
    "        tfidf_df, cv_model, idf_model, normalizer = None, None, None, None\n",
    "    \n",
    "    # 4) Compute item similarities (co-occurrence)\n",
    "    print(f\"\\n\\n{'*'*60}\")\n",
    "    print(f\"PHASE 4: COMPUTE ITEM SIMILARITIES\")\n",
    "    print(f\"{'*'*60}\\n\")\n",
    "    try:\n",
    "        cooccurrences_df = compute_item_similarities_spark(interactions_indexed, item2idx)\n",
    "        print(f\"✓ Item similarities computed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Similarity computation failed: {e}\")\n",
    "        cooccurrences_df = None\n",
    "    \n",
    "    # 5) Build FP-Growth association rules\n",
    "    print(f\"\\n\\n{'*'*60}\")\n",
    "    print(f\"PHASE 5: BUILD ASSOCIATION RULES (FP-GROWTH)\")\n",
    "    print(f\"{'*'*60}\\n\")\n",
    "    rules_df = None\n",
    "    try:\n",
    "        rules_df, freq_itemsets, fpgrowth_model = build_fpgrowth_rules_spark(\n",
    "            interactions_indexed,\n",
    "            min_support=fpgrowth_min_support,\n",
    "            sample_frac=fpgrowth_sample_frac,\n",
    "            min_confidence=0.2\n",
    "        )\n",
    "        print(f\"✓ Rules generated successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ FP-Growth rules generation failed: {e}\")\n",
    "        print(f\"  - Continuing without association rules...\")\n",
    "        rules_df = None\n",
    "    \n",
    "    # 6) Train ALS model\n",
    "    print(f\"\\n\\n{'*'*60}\")\n",
    "    print(f\"PHASE 6: TRAIN ALS COLLABORATIVE FILTERING MODEL\")\n",
    "    print(f\"{'*'*60}\\n\")\n",
    "    als_model = None\n",
    "    try:\n",
    "        als_model, als_data = train_als_model_spark(\n",
    "            interactions_indexed,\n",
    "            factors=als_factors,\n",
    "            regularization=0.01,\n",
    "            iterations=als_iterations,\n",
    "            implicit_prefs=True,\n",
    "            alpha=40.0\n",
    "        )\n",
    "        print(f\"✓ ALS training successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ ALS training failed: {e}\")\n",
    "        als_model = None\n",
    "    \n",
    "    # 7) Build recommender object\n",
    "    print(f\"\\n\\n{'*'*60}\")\n",
    "    print(f\"PHASE 7: INITIALIZE HYBRID RECOMMENDER OBJECT\")\n",
    "    print(f\"{'*'*60}\\n\")\n",
    "    print(f\"[Setup] Creating HybridRecommenderSpark with all components...\")\n",
    "    \n",
    "    hreco = HybridRecommenderSpark(\n",
    "        spark=spark,\n",
    "        interactions_indexed_df=interactions_indexed,\n",
    "        user2idx=user2idx,\n",
    "        item2idx=item2idx,\n",
    "        idx2user=idx2user,\n",
    "        idx2item=idx2item,\n",
    "        track_dict_broadcast=track_dict_broadcast,\n",
    "        als_model=als_model,\n",
    "        tfidf_df=tfidf_df,\n",
    "        rules_df=rules_df,\n",
    "        cooccurrences_df=cooccurrences_df\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ HybridRecommenderSpark object created successfully!\")\n",
    "    print(f\"\\n[COMPONENTS SUMMARY]\")\n",
    "    print(f\"  - Interactions DataFrame: {'✓' if hreco.interactions_indexed_df is not None else '✗'}\")\n",
    "    print(f\"  - ALS model: {'✓' if hreco.als_model is not None else '✗'}\")\n",
    "    print(f\"  - TF-IDF features: {'✓' if hreco.tfidf_df is not None else '✗'}\")\n",
    "    print(f\"  - Association rules: {'✓' if hreco.rules_df is not None else '✗'}\")\n",
    "    print(f\"  - Co-occurrences: {'✓' if hreco.cooccurrences_df is not None else '✗'}\")\n",
    "    print(f\"  - Track dictionary: {'✓' if hreco.track_dict_broadcast is not None else '✗'}\")\n",
    "    print(f\"\\n✓ Pipeline built successfully!\\n\")\n",
    "    \n",
    "    return hreco, rules_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d032d3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "================================================================================\n",
      "STARTING RECOMMENDATION SYSTEM - MAIN ENTRY POINT\n",
      "================================================================================\n",
      "================================================================================\n",
      "\n",
      "[VALIDATION] Checking input files...\n",
      "  - Interactions path: train_triplets.txt\n",
      "  - Tracks CSV path: tracks.csv\n",
      "  ✓ Interactions file exists\n",
      "  ✓ Tracks CSV file exists\n",
      "\n",
      "[BUILDING] Initializing recommendation pipeline...\n",
      "  This will take time and memory - please be patient...\n",
      "\n",
      "\n",
      "\n",
      "############################################################\n",
      "############################################################\n",
      "# HYBRID RECOMMENDATION SYSTEM - FULL PIPELINE INITIALIZATION\n",
      "############################################################\n",
      "############################################################\n",
      "\n",
      "[CONFIG] Pipeline parameters:\n",
      "  - Interactions file: train_triplets.txt\n",
      "  - Tracks CSV file: tracks.csv\n",
      "  - Chunk size: 2,000,000\n",
      "  - FP-Growth min support: 0.001\n",
      "  - FP-Growth sample fraction: 0.2\n",
      "  - USE_IMPLICIT: True\n",
      "  - FP_AVAILABLE: True\n",
      "\n",
      "\n",
      "************************************************************\n",
      "PHASE 1: LOAD TRACK METADATA\n",
      "************************************************************\n",
      "\n",
      "\n",
      "============================================================\n",
      "[STEP 1] Loading tracks metadata from tracks.csv ...\n",
      "============================================================\n",
      "✓ Tracks CSV loaded successfully\n",
      "  - Total records: 617186\n",
      "  - Columns: ['artist', 'timestamp', 'similars', 'tags', 'track_id', 'title']\n",
      "  - Shape: (617186, 6)\n",
      "✓ Tracks CSV loaded successfully\n",
      "  - Total records: 617186\n",
      "  - Columns: ['artist', 'timestamp', 'similars', 'tags', 'track_id', 'title']\n",
      "  - Shape: (617186, 6)\n",
      "  - Memory usage: 1578.33 MB\n",
      "\n",
      "[Processing tags and similars...]\n",
      "  - Memory usage: 1578.33 MB\n",
      "\n",
      "[Processing tags and similars...]\n",
      "✓ Tags processed - sample: []\n",
      "✓ Tags processed - sample: []\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# --- Entry point example (PySpark) ---\n",
    "# ---------------------------\n",
    "\n",
    "# Example usage:\n",
    "# hreco, rules = build_and_run_pipeline_main_spark(\n",
    "#     spark=spark,\n",
    "#     interactions_hdfs_path=\"/data/train_triplets.txt\",\n",
    "#     tracks_hdfs_path=\"/data/lastfm_data.csv\",\n",
    "#     fpgrowth_min_support=0.001,\n",
    "#     fpgrowth_sample_frac=0.2,\n",
    "#     als_factors=64,\n",
    "#     als_iterations=15\n",
    "# )\n",
    "\n",
    "# Test recommendations:\n",
    "# sample_user = next(iter(hreco.user2idx.keys()))\n",
    "# sample_track = next(iter(hreco.item2idx.keys()))\n",
    "# user_recs = hreco.recommend_for_user(sample_user, top_k=10)\n",
    "# similar_tracks = hreco.recommend_similar_song(sample_track, top_k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e450382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
