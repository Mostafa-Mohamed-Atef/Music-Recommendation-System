{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd6e6dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/11 14:20:10 WARN Utils: Your hostname, DESKTOP-P46QK96 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/12/11 14:20:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/11 14:20:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Initialized with Driver Memory: 8g\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, split, udf, array_contains, expr, lit, array\n",
    "from pyspark.sql.types import ArrayType, StringType, DoubleType, IntegerType\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, Tokenizer, HashingTF, IDF, VectorAssembler, Normalizer\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "# --- FIX: Increase spark.driver.memory to handle large .collect() operations ---\n",
    "# Set the driver memory higher (e.g., 8GB) to accommodate the collected track metadata.\n",
    "DRIVER_MEMORY = \"8g\" \n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySparkHybridRecommender\") \\\n",
    "    .config(\"spark.driver.memory\", DRIVER_MEMORY) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Session Initialized with Driver Memory: {DRIVER_MEMORY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69c87a81-5994-44cf-afbc-08f2fb080d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Loading Utilities (PySpark)\n",
    "\n",
    "# Cell 2: Data Loading Utilities (PySpark)\n",
    "\n",
    "def load_track_map(spark: SparkSession, map_path: str) -> pyspark.sql.DataFrame:\n",
    "    \"\"\"Load the Track ID <-> Song ID mapping file and return a DataFrame mapping Song ID to Track ID.\"\"\"\n",
    "    print(f\"[LOAD] Loading Track ID <-> Song ID mapping from {map_path}...\")\n",
    "    \n",
    "    # Trying the literal separator '<SEP>' based on the file content snippet.\n",
    "    df_map = spark.read.csv(\n",
    "        map_path, \n",
    "        sep='<SEP>',  # ASSUMING THE LITERAL DELIMITER IS '<SEP>'\n",
    "        header=False, \n",
    "        inferSchema=True\n",
    "    )\n",
    "\n",
    "    # Temporary debugging step: print schema to see how many columns Spark found\n",
    "    print(\" - Debug Schema (load_track_map):\")\n",
    "    df_map.printSchema()\n",
    "    \n",
    "    # We now expect to find columns _c0, _c1, _c2, _c3\n",
    "    \n",
    "    try:\n",
    "        df_map = df_map.select(\n",
    "            col('_c0').alias('track_id'),\n",
    "            col('_c1').alias('song_id')\n",
    "        ).filter(col('track_id').isNotNull() & col('song_id').isNotNull())\n",
    "    except Exception as e:\n",
    "        print(f\" - FATAL WARNING: Failed to select columns. Please check the delimiter in your file. Error: {e}\")\n",
    "        # Reraise the error to halt execution and let the user fix the file/path.\n",
    "        raise\n",
    "\n",
    "    df_map = df_map.dropDuplicates(['song_id']).dropDuplicates(['track_id'])\n",
    "    \n",
    "    print(f\" - Track mapping loaded: {df_map.count()} records\")\n",
    "    return df_map\n",
    "\n",
    "# ... rest of Cell 2 functions remain the same\n",
    "\n",
    "def load_interactions(spark: SparkSession, interactions_path: str, df_map: pyspark.sql.DataFrame) -> pyspark.sql.DataFrame:\n",
    "    \"\"\"Load and join the interactions file (User, Song ID, Playcount) with the map to get Track IDs.\"\"\"\n",
    "    print(f\"[LOAD] Loading interactions from {interactions_path} and joining with Track IDs...\")\n",
    "    \n",
    "    # Load raw interactions (User ID, Song ID, Play Count)\n",
    "    df_raw = spark.read.csv(interactions_path, sep='\\\\t', header=False, inferSchema=True)\n",
    "    df_raw = df_raw.toDF('user', 'song_id', 'playcount')\n",
    "    df_raw = df_raw.withColumn('playcount', col('playcount').cast(DoubleType()))\n",
    "    \n",
    "    # Join with the mapping file to switch from Song ID (SO*) to Track ID (TR*)\n",
    "    # This join operation should also show a progress bar.\n",
    "    interactions_df = df_raw.join(\n",
    "        df_map,\n",
    "        on='song_id',\n",
    "        how='inner'\n",
    "    ).select('user', 'track_id', 'playcount')\n",
    "    \n",
    "    # Force count to ensure the loading and join jobs run and show progress\n",
    "    count = interactions_df.count()\n",
    "    print(f\" - Final Interactions loaded and joined: {count} records\")\n",
    "    return interactions_df\n",
    "\n",
    "from ast import literal_eval\n",
    "def parse_string_to_list(s):\n",
    "    if not s or s == '[]': return []\n",
    "    try:\n",
    "        # literal_eval safely evaluates a string containing a Python structure (like a list)\n",
    "        return literal_eval(s)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# The UDFs needed for tags and similars remain mostly the same, but now they process\n",
    "# the output of the parser, which is a list of lists/tuples, from the CSV string column.\n",
    "\n",
    "def load_track_metadata(spark: SparkSession, csv_path: str) -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    Load track metadata from the SINGLE CSV file generated by the Pandas script.\n",
    "    \"\"\"\n",
    "    print(f\"[LOAD] Loading track metadata from single CSV file: {csv_path}...\")\n",
    "    \n",
    "    # 1. Read the CSV file\n",
    "    df_raw = spark.read.csv(\n",
    "        csv_path,\n",
    "        header=True, # The CSV has a header row from Pandas\n",
    "        inferSchema=True\n",
    "    )\n",
    "    \n",
    "    # Force count to ensure the read job runs and shows progress\n",
    "    raw_count = df_raw.count()\n",
    "    print(f\" - CSV file loaded: {raw_count} records\")\n",
    "\n",
    "    # 2. Define UDFs for cleaning the serialized columns\n",
    "    parse_list_udf = udf(parse_string_to_list, ArrayType(ArrayType(StringType()))) \n",
    "\n",
    "    # UDFs to extract final tags/similars structure from the parsed lists\n",
    "    def extract_tags(tags_list):\n",
    "        # tags_list is now a list of lists like [['tag', count], ...]\n",
    "        if tags_list is None: return []\n",
    "        try:\n",
    "            # We assume tag is the first element (index 0)\n",
    "            return [t[0] for t in tags_list if t and len(t) > 0] \n",
    "        except Exception:\n",
    "            return []\n",
    "            \n",
    "    def extract_similars(similars_list):\n",
    "        # similars_list is now a list of lists like [['track_id', score], ...]\n",
    "        if similars_list is None: return []\n",
    "        try:\n",
    "            # We assume track_id is the first element (index 0)\n",
    "            return [s[0] for s in similars_list if s and len(s) > 0]\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "    extract_tags_udf = udf(extract_tags, ArrayType(StringType()))\n",
    "    extract_similars_udf = udf(extract_similars, ArrayType(StringType()))\n",
    "\n",
    "    # 3. Apply parsing and cleaning\n",
    "    df_tracks = df_raw.select(\n",
    "        col('track_id'),\n",
    "        col('artist'),\n",
    "        col('title'),\n",
    "        extract_tags_udf(parse_list_udf(col('tags'))).alias('tag_list'),\n",
    "        extract_similars_udf(parse_list_udf(col('similars'))).alias('similar_track_ids')\n",
    "    ).filter(col('track_id').isNotNull())\n",
    "\n",
    "    # 4. Clean up\n",
    "    print(\" - Starting cleaning and dropping duplicates. Look for a Spark job progress bar here.\")\n",
    "    df_tracks = df_tracks.dropDuplicates(['track_id'])\n",
    "    \n",
    "    cleaned_count = df_tracks.count() # Force action here too\n",
    "    print(f\" - Cleaned track metadata: {cleaned_count} unique tracks\")\n",
    "\n",
    "    return df_tracks\n",
    "\n",
    "def load_and_filter_tags(spark: SparkSession, path: str, min_count: int = 10000) -> set:\n",
    "    \"\"\"Load the global tag list and return a set of popular tags.\"\"\"\n",
    "    print(f\"[LOAD] Loading and filtering global tag list from {path}...\")\n",
    "    try:\n",
    "        df_tags = spark.read.csv(path, sep='\\\\t', header=False, inferSchema=True).toDF('tag', 'count')\n",
    "        \n",
    "        # This collect() operation forces the file read job\n",
    "        popular_tags_set = set(df_tags.filter(col('count') >= min_count).select('tag').rdd.flatMap(lambda x: x).collect())\n",
    "        \n",
    "        print(f\" - Filtered tag list size: {len(popular_tags_set):,} (min_count={min_count:,})\")\n",
    "        return popular_tags_set\n",
    "    except Exception as e:\n",
    "        print(f\" - WARNING: Could not load or process tag list at {path}. Proceeding without tag filtering. Error: {e}\")\n",
    "        return set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a157a7f-5713-4abe-9afa-adc45864a82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_als(interactions_df: pyspark.sql.DataFrame) -> tuple:\n",
    "    \"\"\"Indexes user and track IDs to prepare for PySpark MLlib ALS.\"\"\"\n",
    "    print(f\"[PREP] Indexing User and Track IDs for ALS...\")\n",
    "\n",
    "    user_indexer = StringIndexer(inputCol=\"user\", outputCol=\"user_idx\")\n",
    "    track_indexer = StringIndexer(inputCol=\"track_id\", outputCol=\"track_idx\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[user_indexer, track_indexer])\n",
    "    model = pipeline.fit(interactions_df)\n",
    "    indexed_df = model.transform(interactions_df)\n",
    "\n",
    "    indexed_df.cache()\n",
    "    print(f\" - Indexed DataFrame prepared and cached.\")\n",
    "\n",
    "    user_indexer_model = model.stages[0]\n",
    "    track_indexer_model = model.stages[1]\n",
    "\n",
    "    return indexed_df, user_indexer_model, track_indexer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "831259e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pyspark_als(indexed_df: pyspark.sql.DataFrame, factors: int = 64, regularization: float = 0.01, iterations: int = 15):\n",
    "    \"\"\"Trains a PySpark MLlib Alternating Least Squares (ALS) model.\"\"\"\n",
    "    print(f\"[ALS] Training PySpark MLlib ALS Model...\")\n",
    "\n",
    "    als = ALS(\n",
    "        rank=factors,\n",
    "        maxIter=iterations,\n",
    "        regParam=regularization,\n",
    "        userCol=\"user_idx\",\n",
    "        itemCol=\"track_idx\",\n",
    "        ratingCol=\"playcount\",\n",
    "        implicitPrefs=True,\n",
    "        coldStartStrategy=\"drop\",\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    model = als.fit(indexed_df)\n",
    "    print(f\" - ALS Model training complete.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65b74209-bb67-4cea-8ae2-3ebf73ea2c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: build_content_features (FIXED)\n",
    "\n",
    "def build_content_features(tracks_df: pyspark.sql.DataFrame, popular_tags: set, max_features: int = 20000):\n",
    "    \"\"\"\n",
    "    Builds a TF-IDF matrix using track tags (filtered by popular_tags) and title.\n",
    "    \"\"\"\n",
    "    print(f\"[CONTENT] Building Content TF-IDF Features...\")\n",
    "\n",
    "    # --- FIX APPLIED HERE ---\n",
    "    # Corrected access to SparkContext via sparkSession\n",
    "    popular_tags_broadcast = tracks_df.sparkSession.sparkContext.broadcast(popular_tags)    \n",
    "    def filter_and_flatten_tags(tags, title):\n",
    "        if tags is None: tags = []\n",
    "        \n",
    "        if popular_tags_broadcast.value:\n",
    "             # popular_tags_broadcast.value contains the set of popular tags\n",
    "             filtered_tags = [t for t in tags if t in popular_tags_broadcast.value]\n",
    "        else:\n",
    "             filtered_tags = tags\n",
    "             \n",
    "        # Combine filtered tags and title into a single text document\n",
    "        text = \" \".join(filtered_tags) + \" \" + (title if title else \"\")\n",
    "        return text.strip()\n",
    "\n",
    "    filter_and_flatten_tags_udf = udf(filter_and_flatten_tags, StringType())\n",
    "\n",
    "    # 1. Create the text feature column\n",
    "    content_df = tracks_df.withColumn(\n",
    "        \"features_text\",\n",
    "        filter_and_flatten_tags_udf(col('tag_list'), col('title'))\n",
    "    ).filter(col('features_text') != '') # Drop records where feature text is empty\n",
    "\n",
    "    # 2. Tokenize, Hash (TF), and IDF\n",
    "    tokenizer = Tokenizer(inputCol=\"features_text\", outputCol=\"words\")\n",
    "    hashing_tf = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"raw_features\", numFeatures=max_features)\n",
    "    idf = IDF(inputCol=hashing_tf.getOutputCol(), outputCol=\"content_features_vector\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[tokenizer, hashing_tf, idf])\n",
    "    model = pipeline.fit(content_df)\n",
    "    tfidf_df = model.transform(content_df)\n",
    "\n",
    "    # 3. Normalize the vector\n",
    "    normalizer = Normalizer(inputCol=\"content_features_vector\", outputCol=\"normalized_features\", p=2.0)\n",
    "    normalized_tfidf_df = normalizer.transform(tfidf_df)\n",
    "\n",
    "    # Force action to materialize the TF-IDF creation job and show progress\n",
    "    normalized_tfidf_df.cache()\n",
    "    count = normalized_tfidf_df.count()\n",
    "    print(f\" - Content TF-IDF matrix built: {count} records.\")\n",
    "\n",
    "    return normalized_tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87701780-3c21-4228-89d1-4e1c1097ea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cooccurrence_matrix(indexed_df: pyspark.sql.DataFrame, top_k: int = 50):\n",
    "    \"\"\"Calculates item-item co-occurrence scores from user transactions (session-based).\"\"\"\n",
    "    print(f\"[COOCC] Calculating Item-Item Co-occurrence for top {top_k} similars...\")\n",
    "\n",
    "    user_tracks = indexed_df.groupBy('user_idx').agg(expr('collect_list(track_idx)').alias('tracks'))\n",
    "    exploded_tracks = user_tracks.withColumn(\"item1\", explode(\"tracks\"))\n",
    "    \n",
    "    cooccurrences = exploded_tracks.alias('df1').join(\n",
    "        exploded_tracks.alias('df2'),\n",
    "        (col('df1.user_idx') == col('df2.user_idx')) & (col('df1.item1') != col('df2.item1'))\n",
    "    ).select(\n",
    "        col('df1.item1').alias('track1'),\n",
    "        col('df2.item1').alias('track2')\n",
    "    ).groupBy('track1', 'track2').count()\n",
    "\n",
    "    cooccurrences.cache()\n",
    "\n",
    "    from pyspark.sql.window import Window\n",
    "    window_spec = Window.partitionBy(\"track1\").orderBy(col(\"count\").desc())\n",
    "    ranked_coocc = cooccurrences.withColumn(\"rank\", expr(f\"row_number() over (partition by track1 order by count desc)\"))\n",
    "    top_coocc = ranked_coocc.filter(col(\"rank\") <= top_k).drop(\"rank\")\n",
    "\n",
    "    coocc_list = top_coocc.collect()\n",
    "    coocc_dict = defaultdict(list)\n",
    "    for row in coocc_list:\n",
    "        coocc_dict[row.track1].append((row.track2, row['count']))\n",
    "\n",
    "    print(f\" - Co-occurrence calculation complete. {len(coocc_dict)} unique tracks found.\")\n",
    "    return coocc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8de87719",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRecommenderPySpark:\n",
    "    def __init__(self, spark_session, als_model, track_indexer_model, user_indexer_model,\n",
    "                 tracks_df, content_features_df, coocc_dict):\n",
    "        # ... (initialization as before)\n",
    "        self.spark = spark_session\n",
    "        self.als_model = als_model\n",
    "        self.coocc_dict = coocc_dict\n",
    "\n",
    "        self.track_indexer = track_indexer_model\n",
    "        self.user_indexer = user_indexer_model\n",
    "\n",
    "        self.raw_track_ids = track_indexer_model.labels\n",
    "        self.raw_user_ids = user_indexer_model.labels\n",
    "        self.track_id_to_idx = {raw: i for i, raw in enumerate(self.raw_track_ids)}\n",
    "        self.user_id_to_idx = {raw: i for i, raw in enumerate(self.raw_user_ids)}\n",
    "\n",
    "        print(\"[INIT] Building local content feature and track dictionary...\")\n",
    "        self.content_features_dict = {\n",
    "            row.track_id: row.normalized_features for row in content_features_df.select('track_id', 'normalized_features').collect()\n",
    "        }\n",
    "        self.track_metadata = {\n",
    "            row.track_id: {'artist': row.artist, 'title': row.title, 'tag_list': row.tag_list, 'similar_track_ids': row.similar_track_ids}\n",
    "            for row in tracks_df.collect()\n",
    "        }\n",
    "\n",
    "    def get_track_info(self, track_id: str) -> dict:\n",
    "        return self.track_metadata.get(track_id, {'artist': 'Unknown', 'title': 'Unknown', 'tag_list': [], 'similar_track_ids': []})\n",
    "\n",
    "    def recommend_for_user(self, user_id: str, top_k: int = 10, weights: dict = None) -> list:\n",
    "        if weights is None:\n",
    "            weights = {'als': 0.6, 'coocc': 0.4}\n",
    "\n",
    "        if user_id not in self.user_id_to_idx:\n",
    "            print(f\"Cold start user: {user_id}. Returning dummy popular items.\")\n",
    "            return [('TRdummy_pop1', 1.0), ('TRdummy_pop2', 0.9)]\n",
    "\n",
    "        user_idx = self.user_id_to_idx[user_id]\n",
    "        scored = defaultdict(float)\n",
    "\n",
    "        # 1. ALS Recommendations (The primary engine)\n",
    "        if self.als_model is not None:\n",
    "            try:\n",
    "                users_df = self.spark.createDataFrame([(user_idx,)], [self.als_model.getUserCol()])\n",
    "                recs_df = self.als_model.recommendForUserSubset(users_df, top_k*5)\n",
    "\n",
    "                recs_row = recs_df.collect()[0] if recs_df.count() > 0 else None\n",
    "                recs = recs_row.recommendations if recs_row else []\n",
    "\n",
    "                for rec in recs:\n",
    "                    tid = self.raw_track_ids[rec.track_idx]\n",
    "                    scored[tid] += weights.get('als', 0.0) * float(rec.rating)\n",
    "                print(f\" - ALS recommendations processed.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ALS recommendation failed: {e}\")\n",
    "\n",
    "        # 2. Co-occurrence Recommendations (Item-to-Item from user history - simplified)\n",
    "        print(\" - Skipping Co-occurrence recommendation due to need for user history lookup/logic.\")\n",
    "\n",
    "        # Final combination and sorting (based on ALS/other scores)\n",
    "        final = sorted(scored.items(), key=lambda x: x[1], reverse=True)\n",
    "        return final[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0c0e04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 1] Starting Data Loading...\n",
      "[LOAD] Loading Track ID <-> Song ID mapping from data/unique_tracks.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Debug Schema (load_track_map):\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Track mapping loaded: 999056 records\n",
      "[LOAD] Loading interactions from data/kaggle_visible_evaluation_triplets.txt and joining with Track IDs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Final Interactions loaded and joined: 1450933 records\n",
      "[LOAD] Loading track metadata from single CSV file: data/lastfm_data.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - CSV file loaded: 839122 records\n",
      " - Starting cleaning and dropping duplicates. Look for a Spark job progress bar here.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Cleaned track metadata: 825713 unique tracks\n",
      "[LOAD] Loading and filtering global tag list from data/list_of_tags.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:>                                                         (0 + 3) / 3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Filtered tag list size: 86 (min_count=10,000)\n",
      "\n",
      "\n",
      "--- DATA LOADING COMPLETE ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Cell A: Paths & Data Loading (Run this only once)\n",
    "\n",
    "# --- CONFIGURE FILE PATHS HERE ---\n",
    "# 1. THE USER INTERACTION DATA (The file you just found)\n",
    "interactions_path = \"data/kaggle_visible_evaluation_triplets.txt\"    \n",
    "# 2. THE TRACK ID <-> SONG ID MAPPING \n",
    "track_map_path = \"data/unique_tracks.txt\" \n",
    "# 3. THE TRACK METADATA CSV (from your converter script)\n",
    "tracks_data_dir = \"data/lastfm_data.csv\"\n",
    "# 4. THE GLOBAL TAG COUNT FILE\n",
    "tag_list_path = \"data/list_of_tags.txt\"        \n",
    "\n",
    "# -----------------------------------\n",
    "\n",
    "# --- 1. Load Data & Prepare Filters ---\n",
    "print(\"\\n[STEP 1] Starting Data Loading...\")\n",
    "df_track_map = load_track_map(spark, track_map_path)\n",
    "interactions_df = load_interactions(spark, interactions_path, df_track_map)\n",
    "tracks_df = load_track_metadata(spark, tracks_data_dir)\n",
    "popular_tags = load_and_filter_tags(spark, tag_list_path, min_count=10000)\n",
    "\n",
    "print(\"\\n\\n--- DATA LOADING COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70d72170-485b-4ba5-b6e6-65eece2e3c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 2] Starting Preprocessing and Feature Engineering...\n",
      "[PREP] Indexing User and Track IDs for ALS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Indexed DataFrame prepared and cached.\n",
      "[CONTENT] Building Content TF-IDF Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 88:====================================================>   (15 + 1) / 16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Content TF-IDF matrix built: 825713 records.\n",
      "\n",
      "\n",
      "--- PREPROCESSING & FEATURE ENGINEERING COMPLETE ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Cell B: Preprocessing & Feature Engineering\n",
    "\n",
    "print(\"\\n[STEP 2] Starting Preprocessing and Feature Engineering...\")\n",
    "# --- 2. Preprocess for ALS ---\n",
    "indexed_df, user_indexer, track_indexer = preprocess_for_als(interactions_df)\n",
    "\n",
    "# --- 3. Content Features ---\n",
    "# This step now uses the fixed build_content_features function from Cell 5\n",
    "content_features_df = build_content_features(tracks_df, popular_tags, max_features=10000)\n",
    "\n",
    "print(\"\\n\\n--- PREPROCESSING & FEATURE ENGINEERING COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e450382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 3] Starting Model Training...\n",
      "[ALS] Training PySpark MLlib ALS Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/11 13:28:52 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 13:28:58 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 13:29:00 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 13:29:02 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 13:29:04 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 13:29:07 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 13:29:08 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 13:29:10 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 13:29:11 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/12/11 13:29:12 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 13:29:14 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 13:29:16 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "25/12/11 13:29:25 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 13:29:27 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 13:29:35 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 13:29:37 WARN DAGScheduler: Broadcasting large task binary with size 13.6 MiB\n",
      "25/12/11 13:29:43 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 13:29:44 WARN DAGScheduler: Broadcasting large task binary with size 13.6 MiB\n",
      "25/12/11 13:29:49 WARN DAGScheduler: Broadcasting large task binary with size 13.6 MiB\n",
      "25/12/11 13:29:54 WARN DAGScheduler: Broadcasting large task binary with size 13.6 MiB\n",
      "25/12/11 13:30:04 WARN DAGScheduler: Broadcasting large task binary with size 13.6 MiB\n",
      "25/12/11 13:30:06 WARN DAGScheduler: Broadcasting large task binary with size 13.6 MiB\n",
      "25/12/11 13:30:11 WARN DAGScheduler: Broadcasting large task binary with size 13.6 MiB\n",
      "25/12/11 13:30:58 WARN DAGScheduler: Broadcasting large task binary with size 13.6 MiB\n",
      "25/12/11 13:31:08 WARN DAGScheduler: Broadcasting large task binary with size 13.6 MiB\n",
      "25/12/11 13:31:20 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "25/12/11 13:31:26 WARN DAGScheduler: Broadcasting large task binary with size 13.6 MiB\n",
      "25/12/11 13:31:30 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "25/12/11 13:31:37 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "25/12/11 13:31:45 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "25/12/11 13:31:50 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "25/12/11 13:31:55 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "25/12/11 13:32:02 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "25/12/11 13:32:06 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "25/12/11 13:32:11 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "25/12/11 13:32:14 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "25/12/11 13:32:23 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "25/12/11 13:32:25 WARN DAGScheduler: Broadcasting large task binary with size 13.8 MiB\n",
      "25/12/11 13:32:30 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "25/12/11 13:32:32 WARN DAGScheduler: Broadcasting large task binary with size 13.8 MiB\n",
      "25/12/11 13:32:39 WARN DAGScheduler: Broadcasting large task binary with size 13.8 MiB\n",
      "25/12/11 13:32:40 WARN DAGScheduler: Broadcasting large task binary with size 13.8 MiB\n",
      "25/12/11 13:32:45 WARN DAGScheduler: Broadcasting large task binary with size 13.8 MiB\n",
      "25/12/11 13:33:03 WARN DAGScheduler: Broadcasting large task binary with size 13.8 MiB\n",
      "25/12/11 13:33:09 WARN DAGScheduler: Broadcasting large task binary with size 13.8 MiB\n",
      "25/12/11 13:33:10 WARN DAGScheduler: Broadcasting large task binary with size 13.8 MiB\n",
      "25/12/11 13:33:15 WARN DAGScheduler: Broadcasting large task binary with size 13.8 MiB\n",
      "25/12/11 13:33:18 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "25/12/11 13:33:24 WARN DAGScheduler: Broadcasting large task binary with size 13.8 MiB\n",
      "25/12/11 13:33:28 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "25/12/11 13:33:33 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "25/12/11 13:33:36 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "25/12/11 13:33:42 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "25/12/11 13:33:44 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "25/12/11 13:33:49 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "25/12/11 13:33:50 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "25/12/11 13:33:56 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "25/12/11 13:33:58 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "25/12/11 13:34:05 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "25/12/11 13:34:08 WARN DAGScheduler: Broadcasting large task binary with size 14.0 MiB\n",
      "25/12/11 13:34:14 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "25/12/11 13:34:16 WARN DAGScheduler: Broadcasting large task binary with size 14.0 MiB\n",
      "25/12/11 13:34:20 WARN DAGScheduler: Broadcasting large task binary with size 14.0 MiB\n",
      "25/12/11 13:34:26 WARN DAGScheduler: Broadcasting large task binary with size 14.0 MiB\n",
      "25/12/11 13:34:31 WARN DAGScheduler: Broadcasting large task binary with size 14.0 MiB\n",
      "25/12/11 13:34:33 WARN DAGScheduler: Broadcasting large task binary with size 14.0 MiB\n",
      "25/12/11 13:34:40 WARN DAGScheduler: Broadcasting large task binary with size 14.0 MiB\n",
      "25/12/11 13:34:46 WARN DAGScheduler: Broadcasting large task binary with size 14.0 MiB\n",
      "25/12/11 13:34:52 WARN DAGScheduler: Broadcasting large task binary with size 14.0 MiB\n",
      "25/12/11 13:34:53 WARN DAGScheduler: Broadcasting large task binary with size 14.0 MiB\n",
      "25/12/11 13:34:58 WARN DAGScheduler: Broadcasting large task binary with size 14.0 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - ALS Model training complete.\n",
      "[COOCC] Sampling users for faster co-occurrence calculation...\n",
      "[COOCC] Calculating Item-Item Co-occurrence for top 50 similars...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/11 13:35:06 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 13:35:06 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 13:35:12 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 13:35:12 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 13:35:13 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 13:35:14 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 13:35:16 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 13:35:18 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 13:35:21 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 13:35:23 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 13:35:27 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Co-occurrence calculation complete. 52467 unique tracks found.\n",
      "\n",
      "[STEP 4] Initializing Hybrid Recommender System...\n",
      "[INIT] Building local content feature and track dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- MODEL TRAINING & SYSTEM INITIALIZATION COMPLETE ---\n"
     ]
    }
   ],
   "source": [
    "# Cell C: Model Training & Recommender Initialization\n",
    "\n",
    "print(\"\\n[STEP 3] Starting Model Training...\")\n",
    "# --- 4. Collaborative Filtering Model (ALS) ---\n",
    "als_model = train_pyspark_als(indexed_df, factors=64, regularization=0.01, iterations=15)\n",
    "\n",
    "# --- 5. Item-Item Co-occurrence ---\n",
    "# --- 5. Item-Item Co-occurrence (Optimized by sampling users) ---\n",
    "print(\"[COOCC] Sampling users for faster co-occurrence calculation...\")\n",
    "# Collect unique users, sample 10% of their IDs\n",
    "user_sample = indexed_df.select('user_idx').distinct().sample(False, 0.1, seed=42)\n",
    "# Filter the original indexed_df to only include the sampled users\n",
    "sampled_indexed_df = indexed_df.join(user_sample, 'user_idx', 'inner')\n",
    "\n",
    "coocc_dict = build_cooccurrence_matrix(sampled_indexed_df, top_k=50)\n",
    "# --- 6. Initialize Hybrid Recommender ---\n",
    "print(\"\\n[STEP 4] Initializing Hybrid Recommender System...\")\n",
    "hreco = HybridRecommenderPySpark(\n",
    "    spark_session=spark,\n",
    "    als_model=als_model,\n",
    "    track_indexer_model=track_indexer,\n",
    "    user_indexer_model=user_indexer,\n",
    "    tracks_df=tracks_df,\n",
    "    content_features_df=content_features_df,\n",
    "    coocc_dict=coocc_dict\n",
    ")\n",
    "\n",
    "print(\"\\n\\n--- MODEL TRAINING & SYSTEM INITIALIZATION COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cdf62b3-35cc-4e7f-97e4-1dc320499984",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'indexed_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell D: Test Recommendations\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# --- 7. Test Recommendations ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m sample_user = \u001b[43mindexed_df\u001b[49m.select(\u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m).limit(\u001b[32m1\u001b[39m).collect()[\u001b[32m0\u001b[39m].user\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[TEST 1] Finding User Recommendations (PySpark Hybrid System)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'indexed_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell D: Test Recommendations\n",
    "\n",
    "# --- 7. Test Recommendations ---\n",
    "sample_user = indexed_df.select('user').limit(1).collect()[0].user\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"[TEST 1] Finding User Recommendations (PySpark Hybrid System)\")\n",
    "print(f\"  - User ID: {sample_user}\")\n",
    "recs = hreco.recommend_for_user(sample_user, top_k=5)\n",
    "print(\"âœ“ Recommendations found:\")\n",
    "for i, (track_id, score) in enumerate(recs, 1):\n",
    "    info = hreco.get_track_info(track_id)\n",
    "    print(f\"    {i:2d}. Track: {info['title']:25s} | Artist: {info['artist']:25s} | Score: {score:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"System execution complete.\")\n",
    "\n",
    "# Optional: Stop the Spark session when done with all work\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ac3efe3-bba0-4639-82f7-7a58dd653fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Evaluation Utility Functions (Without scikit-learn)\n",
    "\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql.types import StringType\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm # Used to show progress bar during evaluation\n",
    "\n",
    "def split_data_temporal(interactions_df: pyspark.sql.DataFrame, split_ratio: float = 0.8):\n",
    "    \"\"\"\n",
    "    Splits the interactions data into training and testing sets by randomly splitting users.\n",
    "    (Custom implementation replacing sklearn.model_selection.train_test_split)\n",
    "    \n",
    "    Returns:\n",
    "        train_df, test_df (PySpark DataFrames)\n",
    "    \"\"\"\n",
    "    print(\"[EVAL] Splitting interaction data for offline evaluation...\")\n",
    "    \n",
    "    # 1. Collect unique users \n",
    "    users = [row.user for row in interactions_df.select('user').distinct().collect()]\n",
    "    \n",
    "    # --- Custom User Split using NumPy ---\n",
    "    np.random.seed(42) # Set seed for reproducibility\n",
    "    \n",
    "    # Shuffle the list of users\n",
    "    shuffled_users = np.random.permutation(users)\n",
    "    \n",
    "    # Calculate the split index\n",
    "    train_size = int(len(shuffled_users) * split_ratio)\n",
    "    \n",
    "    # Split the users\n",
    "    train_users = shuffled_users[:train_size].tolist()\n",
    "    test_users = shuffled_users[train_size:].tolist()\n",
    "    # ------------------------------------\n",
    "    \n",
    "    # Convert lists to PySpark DataFrames for joining\n",
    "    # Note: We still need 'StringType' imported if the original user IDs were strings.\n",
    "    train_users_df = interactions_df.sparkSession.createDataFrame(train_users, StringType()).toDF('user')\n",
    "    test_users_df = interactions_df.sparkSession.createDataFrame(test_users, StringType()).toDF('user')\n",
    "\n",
    "    # 2. Split the original interactions DataFrame\n",
    "    train_df = interactions_df.join(train_users_df, 'user', 'inner')\n",
    "    test_df = interactions_df.join(test_users_df, 'user', 'inner')\n",
    "\n",
    "    print(f\" - Train set size: {train_df.count()} interactions\")\n",
    "    print(f\" - Test set size: {test_df.count()} interactions\")\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "def calculate_ranking_metrics(predictions, ground_truth, k: int):\n",
    "    \"\"\"\n",
    "    Calculates Precision@K and Recall@K for a list of predictions against ground truth.\n",
    "    \"\"\"\n",
    "    if not ground_truth:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    predicted_k = set(predictions[:k])\n",
    "    hits = len(predicted_k.intersection(ground_truth))\n",
    "    \n",
    "    # Precision@K: (Relevant items in top K) / (Total items in top K)\n",
    "    precision = hits / k\n",
    "    \n",
    "    # Recall@K: (Relevant items in top K) / (Total relevant items)\n",
    "    recall = hits / len(ground_truth)\n",
    "    \n",
    "    return precision, recall\n",
    "\n",
    "def evaluate_model_performance(spark, model_recommender, test_df, k: int):\n",
    "    \"\"\"\n",
    "    Main evaluation routine to measure model performance on a test set.\n",
    "    Note: It samples a maximum of 100 users for quick evaluation based on previous optimization.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[EVAL] Starting evaluation for K={k}...\")\n",
    "    \n",
    "    test_data_pd = test_df.toPandas()\n",
    "    ground_truth = defaultdict(set)\n",
    "    for index, row in test_data_pd.iterrows():\n",
    "        ground_truth[row['user']].add(row['track_id'])\n",
    "        \n",
    "    test_users = list(ground_truth.keys())\n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "\n",
    "    # Use 100 users as the sampled size for faster results\n",
    "    MAX_TEST_USERS = min(len(test_users), 100) \n",
    "    \n",
    "    print(f\" - Generating Top-{k} recommendations for {MAX_TEST_USERS:,} test users...\")\n",
    "    \n",
    "    # Sample the test users\n",
    "    np.random.seed(42) # for reproducibility\n",
    "    test_users_sample = np.random.choice(test_users, size=MAX_TEST_USERS, replace=False)\n",
    "\n",
    "    for user_id in tqdm(test_users_sample, desc=f\"Evaluating on {MAX_TEST_USERS} users\"):\n",
    "        relevant_tracks = ground_truth[user_id]\n",
    "        if len(relevant_tracks) < 1:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            predictions = [t[0] for t in model_recommender.recommend_for_user(user_id, top_k=k)]\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        precision, recall = calculate_ranking_metrics(predictions, relevant_tracks, k)\n",
    "        \n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "\n",
    "    mean_precision = np.mean(all_precisions)\n",
    "    mean_recall = np.mean(all_recalls)\n",
    "\n",
    "    print(f\"\\n--- Evaluation Results (N={len(all_precisions)} users) ---\")\n",
    "    print(f\"Mean Precision@{k}: {mean_precision:.4f}\")\n",
    "    print(f\"Mean Recall@{k}:    {mean_recall:.4f}\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "\n",
    "    return mean_precision, mean_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc282f6d-2b44-4373-bbf9-a3b68fab0bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] Loading Track ID <-> Song ID mapping from data/unique_tracks.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Debug Schema (load_track_map):\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Track mapping loaded: 999056 records\n",
      "[LOAD] Loading interactions from data/kaggle_visible_evaluation_triplets.txt and joining with Track IDs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Final Interactions loaded and joined: 1450933 records\n",
      "[LOAD] Loading track metadata from single CSV file: data/lastfm_data.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - CSV file loaded: 839122 records\n",
      " - Starting cleaning and dropping duplicates. Look for a Spark job progress bar here.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Cleaned track metadata: 825713 unique tracks\n",
      "[LOAD] Loading and filtering global tag list from data/list_of_tags.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Filtered tag list size: 86 (min_count=10,000)\n",
      "[PREP] Indexing User and Track IDs for ALS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Indexed DataFrame prepared and cached.\n",
      "[EVAL] Splitting interaction data for offline evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Train set size: 1159142 interactions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Test set size: 291791 interactions\n",
      "[CONTENT] Building Content TF-IDF Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Content TF-IDF matrix built: 825713 records.\n",
      "[ALS] Training PySpark MLlib ALS Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/11 14:40:25 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 14:40:30 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 14:40:31 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 14:40:33 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 14:40:35 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 14:40:37 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 14:40:38 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 14:40:39 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 14:40:40 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/12/11 14:40:41 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 14:40:44 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 14:40:51 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "25/12/11 14:40:56 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 14:41:01 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 14:41:05 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 14:41:12 WARN DAGScheduler: Broadcasting large task binary with size 13.6 MiB\n",
      "25/12/11 14:41:17 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 14:41:25 WARN DAGScheduler: Broadcasting large task binary with size 13.6 MiB\n",
      "25/12/11 14:41:29 WARN DAGScheduler: Broadcasting large task binary with size 13.6 MiB\n",
      "25/12/11 14:41:31 WARN DAGScheduler: Broadcasting large task binary with size 13.6 MiB\n",
      "25/12/11 14:41:36 WARN DAGScheduler: Broadcasting large task binary with size 13.6 MiB\n",
      "25/12/11 14:41:45 WARN DAGScheduler: Broadcasting large task binary with size 13.6 MiB\n",
      "25/12/11 14:41:49 WARN DAGScheduler: Broadcasting large task binary with size 13.6 MiB\n",
      "25/12/11 14:42:03 WARN DAGScheduler: Broadcasting large task binary with size 13.6 MiB\n",
      "25/12/11 14:42:08 WARN DAGScheduler: Broadcasting large task binary with size 13.6 MiB\n",
      "25/12/11 14:42:16 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "25/12/11 14:42:20 WARN DAGScheduler: Broadcasting large task binary with size 13.6 MiB\n",
      "25/12/11 14:42:30 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "25/12/11 14:42:36 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "25/12/11 14:42:45 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "25/12/11 14:42:49 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "25/12/11 14:42:59 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "25/12/11 14:43:04 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "25/12/11 14:43:13 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "25/12/11 14:43:16 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "25/12/11 14:43:25 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "25/12/11 14:43:29 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "25/12/11 14:43:36 WARN DAGScheduler: Broadcasting large task binary with size 13.8 MiB\n",
      "25/12/11 14:43:42 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "25/12/11 14:43:48 WARN DAGScheduler: Broadcasting large task binary with size 13.8 MiB\n",
      "25/12/11 14:43:52 WARN DAGScheduler: Broadcasting large task binary with size 13.8 MiB\n",
      "25/12/11 14:43:59 WARN DAGScheduler: Broadcasting large task binary with size 13.8 MiB\n",
      "25/12/11 14:44:02 WARN DAGScheduler: Broadcasting large task binary with size 13.8 MiB\n",
      "25/12/11 14:44:09 WARN DAGScheduler: Broadcasting large task binary with size 13.8 MiB\n",
      "25/12/11 14:44:15 WARN DAGScheduler: Broadcasting large task binary with size 13.8 MiB\n",
      "25/12/11 14:44:22 WARN DAGScheduler: Broadcasting large task binary with size 13.8 MiB\n",
      "25/12/11 14:44:26 WARN DAGScheduler: Broadcasting large task binary with size 13.8 MiB\n",
      "25/12/11 14:44:32 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "25/12/11 14:44:36 WARN DAGScheduler: Broadcasting large task binary with size 13.8 MiB\n",
      "25/12/11 14:44:42 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "25/12/11 14:44:46 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "25/12/11 14:44:56 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "25/12/11 14:45:01 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "25/12/11 14:45:06 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "25/12/11 14:45:09 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "25/12/11 14:45:15 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "25/12/11 14:45:19 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "25/12/11 14:45:26 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "25/12/11 14:45:30 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "25/12/11 14:45:35 WARN DAGScheduler: Broadcasting large task binary with size 14.0 MiB\n",
      "25/12/11 14:45:39 WARN DAGScheduler: Broadcasting large task binary with size 13.9 MiB\n",
      "25/12/11 14:45:46 WARN DAGScheduler: Broadcasting large task binary with size 14.0 MiB\n",
      "25/12/11 14:45:50 WARN DAGScheduler: Broadcasting large task binary with size 14.0 MiB\n",
      "25/12/11 14:46:00 WARN DAGScheduler: Broadcasting large task binary with size 14.0 MiB\n",
      "25/12/11 14:46:05 WARN DAGScheduler: Broadcasting large task binary with size 14.0 MiB\n",
      "25/12/11 14:46:14 WARN DAGScheduler: Broadcasting large task binary with size 14.0 MiB\n",
      "25/12/11 14:46:17 WARN DAGScheduler: Broadcasting large task binary with size 14.0 MiB\n",
      "25/12/11 14:46:26 WARN DAGScheduler: Broadcasting large task binary with size 14.0 MiB\n",
      "25/12/11 14:46:33 WARN DAGScheduler: Broadcasting large task binary with size 14.0 MiB\n",
      "25/12/11 14:46:34 WARN DAGScheduler: Broadcasting large task binary with size 14.0 MiB\n",
      "25/12/11 14:46:38 WARN DAGScheduler: Broadcasting large task binary with size 14.0 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - ALS Model training complete.\n",
      "[COOCC] Sampling users for faster co-occurrence calculation...\n",
      "[COOCC] Calculating Item-Item Co-occurrence for top 50 similars...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/11 14:46:44 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 14:46:45 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 14:46:47 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 14:46:47 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 14:46:48 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 14:46:48 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 14:46:51 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 14:46:52 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 14:46:54 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 14:46:55 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "25/12/11 14:46:57 WARN DAGScheduler: Broadcasting large task binary with size 13.5 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Co-occurrence calculation complete. 52467 unique tracks found.\n",
      "\n",
      "[STEP 6] Initializing Hybrid Recommender System...\n",
      "[INIT] Building local content feature and track dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 986:>                                                       (0 + 8) / 15]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- CONFIGURE FILE PATHS HERE ---\n",
    "    # 1. THE USER INTERACTION DATA\n",
    "    interactions_path = \"data/kaggle_visible_evaluation_triplets.txt\"    \n",
    "    # 2. THE TRACK ID <-> SONG ID MAPPING\n",
    "    track_map_path = \"data/unique_tracks.txt\" \n",
    "    # 3. THE TRACK METADATA JSONS\n",
    "    tracks_data_dir = \"data/lastfm_data.csv\"\n",
    "    # 4. THE GLOBAL TAG COUNT FILE\n",
    "    tag_list_path = \"data/list_of_tags.txt\"        \n",
    "    \n",
    "    # -----------------------------------\n",
    "\n",
    "    # --- 1. Load Data & Prepare Filters ---\n",
    "    df_track_map = load_track_map(spark, track_map_path)\n",
    "    interactions_df = load_interactions(spark, interactions_path, df_track_map)\n",
    "    tracks_df = load_track_metadata(spark, tracks_data_dir)\n",
    "    popular_tags = load_and_filter_tags(spark, tag_list_path, min_count=10000)\n",
    "\n",
    "    # --- 2. Preprocess for ALS & Split Data ---\n",
    "    indexed_df, user_indexer, track_indexer = preprocess_for_als(interactions_df)\n",
    "    \n",
    "    # Split the original (unindexed) interactions data for evaluation\n",
    "    train_interactions_df, test_interactions_df = split_data_temporal(interactions_df, split_ratio=0.8)\n",
    "\n",
    "    # --- 3. Content Features ---\n",
    "    content_features_df = build_content_features(tracks_df, popular_tags, max_features=10000)\n",
    "\n",
    "    # --- 4. Collaborative Filtering Model ---\n",
    "    als_model = train_pyspark_als(indexed_df, factors=64, regularization=0.01, iterations=15)\n",
    "\n",
    "    # --- 5. Item-Item Co-occurrence (Optimized by sampling users) ---\n",
    "    print(\"[COOCC] Sampling users for faster co-occurrence calculation...\")\n",
    "    # Sample 10% of users for the co-occurrence matrix calculation\n",
    "    user_sample = indexed_df.select('user_idx').distinct().sample(False, 0.1, seed=42)\n",
    "    sampled_indexed_df = indexed_df.join(user_sample, 'user_idx', 'inner')\n",
    "\n",
    "    coocc_dict = build_cooccurrence_matrix(sampled_indexed_df, top_k=50)\n",
    "\n",
    "    # --- 6. Initialize Hybrid Recommender ---\n",
    "    print(\"\\n[STEP 6] Initializing Hybrid Recommender System...\")\n",
    "    hreco = HybridRecommenderPySpark(\n",
    "        spark_session=spark,\n",
    "        als_model=als_model,\n",
    "        track_indexer_model=track_indexer,\n",
    "        user_indexer_model=user_indexer,\n",
    "        tracks_df=tracks_df,\n",
    "        content_features_df=content_features_df,\n",
    "        coocc_dict=coocc_dict\n",
    "    )\n",
    "    print(\"\\n\\n--- MODEL TRAINING & SYSTEM INITIALIZATION COMPLETE ---\")\n",
    "    \n",
    "    # --- 7. Evaluate the Model ---\n",
    "    K = 10 \n",
    "    print(f\"\\n[EVAL] Starting Model Evaluation (Precision@{K} and Recall@{K})\")\n",
    "    evaluate_model_performance(spark, hreco, test_interactions_df, k=K)\n",
    "\n",
    "    # --- 8. Final Cleanup ---\n",
    "    # The existing test recommendation block is removed/commented out to prioritize the full evaluation.\n",
    "    \n",
    "    # Stop the Spark session\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b861d88-ffcc-44dc-a6f4-81bfe3f3ece3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e74546-9d96-4e02-8f66-8625f42a9223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43ce323-f7bc-433b-8d6b-a7f59c9308ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b28bf6-0701-40f9-ad2d-f2d325fa5fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
